# Installation of VScode plugins to use local LLMs

To follow next steps you have to run local [ollama](README.md) and [Visual Studio Code](https://code.visualstudio.com/).

## Continue extension for VScode
Install [Continue plugin](continue.dev)

Choose your LLM model
- Add Model
    - Add new model -> Ollama
    - Configure provider -> Autodetect


You should have both models which you downloaded in [README](README.md).  

Models to choose:
- `Ollama - codestral:latest`
- `Ollama - llama3.1:latest`

Choose one of them and try coding! 

