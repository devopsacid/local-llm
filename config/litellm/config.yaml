model_list:
  - model_name: llama3.1
    litellm_params:
      model: ollama/llama3.1
      api_base: http://ollama:11434
      stream: True
  - model_name: mistral
    litellm_params:
      model: ollama/mistral
      api_base: http://ollama:11434
      stream: True
  - model_name: codellama
    litellm_params:
      model: ollama/codellama
      api_base: http://ollama:11434
      stream: True

# module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py
litellm_settings: 

  drop_params: True
  max_budget: 100 
  budget_duration: 30d

  cache: True          # set cache responses to True, litellm defaults to using a redis cache
  cache_params:         # cache_params are optional
    type: "redis"  # The type of cache to initialize. Can be "local" or "redis". Defaults to "local".
    # Optional configurations
    supported_call_types: ["acompletion", "completion", "embedding", "aembedding"] # defaults to all litellm call types

general_settings: 
  master_key: sk-1234 # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)
  # database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # [OPTIONAL] use for token-based auth to proxy

environment_variables:
  #settings for using redis caching
  REDIS_HOST: redis
  REDIS_PORT: "16337"
  REDIS_PASSWORD: ggUqxbMutewgAqTt1462
  #
